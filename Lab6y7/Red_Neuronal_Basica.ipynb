{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Simple. Ejemplo minimalista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar las librerías relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Para graficar en 3-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar datos al azar para entrenar al modelo\n",
    "\n",
    "Trabajaremos con dos variables de entrada, las x1 y x2 en nuestros ejemplos anteriores. Se generan al azar a partir de una distribución uniforme.\n",
    "\n",
    "Se creará una matriz con estas dos variables.  La matriz X del modelo lineal y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Por facilidad, declaramos una variable que indique el tamaño del conjunto \n",
    "#      de datos de entrenamiento.\n",
    "observaciones = 1000\n",
    "\n",
    "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
    "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
    "\n",
    "X = np.column_stack((x1,x2))\n",
    "\n",
    "# Verificar la forma de la matriz \n",
    "# Debiera ser n x k, donde n es el número de observaciones, y k es el número de variables.\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar las metas a las que debemos apuntar\n",
    "\n",
    "Inventaremos una función f(x1, x2) = 2 * x1 - 3 * x2 + 5 + <ruido pequeño>.  El ruido es para hacerlo más realista.\n",
    "\n",
    "Utilizaremos la metodología de ML, y veremos si el algoritmo la ha aprendido.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
    "\n",
    "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
    "\n",
    "# Veamos las dimensiones. Deben ser n x m, donde m es el número de variables de salida.\n",
    "print (metas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar los datos a usar para el entrenamiento\n",
    "\n",
    "La idea es ver que haya una fuerte tendencia que nuestro modelo debe aprender a reproducir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(metas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1N = x1.reshape(observaciones,)\n",
    "x2N = x2.reshape(observaciones,)\n",
    "metasN = metas.reshape(observaciones,)\n",
    "\n",
    "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
    "\n",
    "fig.update_layout(\n",
    "    width = 500,\n",
    "    height = 500,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inicializar variables\n",
    "\n",
    "Inicializaremos los pesos y sesgos, al azar, dentro de un rango inicial pequeño.  Es posible \"jugar\" con este valor pero no es recomendable ya que el uso de rangos iniciales altos inhibe el aprendizaje por parte del algoritmo\n",
    "\n",
    "Los pesos son de dimensiones k x m, donde k es el numero de variables de entrada y m es el número de variables de salida.  \n",
    "\n",
    "Como solo hay una salida, el sesgo es de tamaño 1, y es un escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rango_inicial = 0.1\n",
    "\n",
    "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
    "\n",
    "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
    "\n",
    "#Veamos cómo fueron inicializados.\n",
    "print (pesos)\n",
    "print (sesgos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asignar la tasa de aprendizaje (Eta)\n",
    "\n",
    "Se asigna un a tasa de aprendizaje pequeña.  Para este ejemplo funciona bien 0.02.  Vale la pena \"jugar\" con este valor para ver los resultados de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo\n",
    "\n",
    "Usaremos un valor de 100 para iterar sobre el conjunto de datos de entrenamiento.  Ese valor funciona bastante bien con la tasa de aprendizaje de 0.02.  Cómo saber el número adecuado de iteraciones es algo que veremos en futuras sesiones, pero generalmente una tasa de aprendizaje baja requiere de más iteraciones que una más alta.  Sin embargo hay que tener en mente que una tasa de aprendizaje alta puede causar que la pérdida \"Loss\" diverja a infinito, en vez de converger a cero (0)\n",
    "\n",
    "Usaremos la función de pérdida L2-norm, pero dividido por 2, para ser consistente con la clase.  Es más, también lo dividiremos por el número de observaciones para obtener un promedio de pérdida por observación.  Hablamos en clase sobre la posiilidad de modificar esta función una vez no se pierda la característica de ser más baja para los resultados mejores, y vice versa.\n",
    "\n",
    "Imprimimos la función de pérdida (loss) en cada iteración, para ver si está decreciendo como se desea.\n",
    "\n",
    "Otro pequeño truco es escalar las deltas de la misma manera que se hizo con la función de pérdida.  De esta forma la tasa de aprendizaje es independiente del número de muestras (samples u observaciones).  De nuevo esto no cambia el principio, solo hace más fácil la selección de una tasa única de aprendizaje. \n",
    "\n",
    "Finalmente aplicamos la regla de actualización del decenso de gradiente.\n",
    "\n",
    "Ojo!  los pesos son 2x1, la tasa de aprendizaje es 1x1 (escalar), las entradas son 1000x2, y las deltas escaladas son 1000x1.  Necesitamos obtener la transpuesta de las entradas para que no hayan problemas de dimensión en las operaciones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range (100):\n",
    "    \n",
    "    # Esta es la ecuacion del modelo lineal: y = xw + b \n",
    "    y = np.dot(X, pesos) + sesgos\n",
    "    \n",
    "    # Las deltas son las diferencias entre las salidas y las metas (targets)\n",
    "    # deltas es un vector 1000 x 1\n",
    "    deltas = y - metas\n",
    "        \n",
    "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
    "    \n",
    "    print(perdida)\n",
    "    \n",
    "    deltas_escaladas = deltas / observaciones\n",
    "      \n",
    "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
    "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
    "    \n",
    "    # Los pesos son actualizados en una forma de algebra lineal(una matriz menos otra)\n",
    "    # Sin embargo, los sesgos en este caso son solo un número (solo estamos calculando una salida), \n",
    "    #       es necesario transformar las deltas a un escalar.      \n",
    "    # Ambas lineas son consistentes con la metodología de decenso de gradiente-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desplegamos los pesos y el sesgo para ver si funcionaron correctamente.\n",
    "\n",
    "Por el diseño de nuestro datos, los pesos debieran ser 2 y -3, y el sesgo: 5\n",
    "\n",
    "**NOTA:**  Si aún no están los valores correctos, puede que aún estén convergiendo y sea necesario iterar más veces.  Para esto solo se requiere ejecutar la celda anterior cuantas veces sea requerido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pesos, sesgos)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar las últimas salidas vrs las metas (targets)\n",
    "\n",
    "Como son las últimas, luego del entrenamiento, representan el modelo final de exactitud.  Entre más cercana esté esta gráfica a una línea de 45 grados, más cercanas están las salidas y metas.\n",
    "\n",
    "Como este ejemplo es pequeño, es posible hacerlo, en los problemas que veremos más tarde en la clase, esto ya no sería posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yN = y.reshape(observaciones,)\n",
    "metasN = metas.reshape(observaciones,)\n",
    "fig = px.scatter(x = yN, y =  metasN)\n",
    "\n",
    "fig.update_layout(\n",
    "    width = 400,\n",
    "    height = 400,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
